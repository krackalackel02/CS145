{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "62e176da",
      "metadata": {
        "id": "62e176da"
      },
      "source": [
        "# CS145 Introduction to Data Mining - Assignment 2  \n",
        "## Deadline: 11:59PM, April 21, 2025\n",
        "\n",
        "## Instructions\n",
        "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
        "\n",
        "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs or explanations. Your answers should include sufficient steps for the mathematical derivations.\n",
        "2. **Coding Problems:** These involve practical coding tasks. You may need to complete code segments provided in the notebooks (marked with **TODO** blocks) or write code from scratch. Please ensure your code is well-commented and executable.\n",
        "\n",
        "### Formatting & Submission\n",
        "- Use Markdown bullet points to format text answers, and use LaTeX for mathematical equations, e.g. `$\\frac{\\partial f}{\\partial x}$`, rather than plain text.\n",
        "- Submit your solutions through GradeScope in BruinLearn, **upload the runned ipynb file directly**, do not export as PDF.\n",
        "- Late submissions are allowed up to 24 hours post-deadline with a penalty factor of\n",
        "  $$\n",
        "    \\mathbf{1}(t \\le 24) \\exp\\left(-\\frac{\\ln(2)}{12} t\\right)\n",
        "  $$\n",
        "  where $t$ is the number of hours past the deadline.\n",
        "\n",
        "### Collaboration & Integrity\n",
        "- Collaboration is encouraged. However, the final submitted work must be *your own*.  \n",
        "- Acknowledge any collaborators or external sources (including websites, textbooks, GitHub repositories, etc.).\n",
        "- **Any** suspicious cases of academic misconduct will be reported to The Office of the Dean of Students."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3936ba0",
      "metadata": {
        "id": "b3936ba0"
      },
      "source": [
        "# Part 1: Write-up (60 points)\n",
        "\n",
        "### 1. k-Nearest Neighbors Fundamentals (15 points)\n",
        "1. **Bias-Variance Intuition** (5 points)  \n",
        "   When the number of neighbors $k$ in $k$-Nearest Neighbors is increased, does the model become more complex or simpler? Does the model variance increase or decrease? Briefly justify your answer in terms of bias and variance trade-off.\n",
        "\n",
        "2. **Curse of Dimensionality and Distance Concentration** (10 points)  \n",
        "   Consider points randomly sampled from a $ d $-dimensional unit hypercube $[0,1]^d$.  \n",
        "   1. **(2 pts)** Let $X_1$ and $X_2$ be two random points in $[0,1]^d$. Express the expected *squared* Euclidean distance, $\\mathbb{E}[(X_1 - X_2)^2]$, between these points in terms of $d$.  \n",
        "   2. **(2 pts)** Similarly, derive $\\mathrm{Var}[(X_1 - X_2)^2]$.  \n",
        "   3. **(3 pts)** Show that as $d$ increases, the ratio of the standard deviation to the mean goes to 0:  \n",
        "      $$\n",
        "      \\lim_{d \\to \\infty} \\frac{\\sqrt{\\mathrm{Var}[(X_1 - X_2)^2]}}{\\mathbb{E}[(X_1 - X_2)^2]} = 0.\n",
        "      $$\n",
        "   4. **(2 pts)** Explain why distances in high dimensions become concentrated around their mean value.  \n",
        "   5. **(1 pt)** Conclude why this complicates finding truly *nearest* neighbors in high-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf5f586",
      "metadata": {
        "id": "bcf5f586"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e43ead",
      "metadata": {
        "id": "d5e43ead"
      },
      "source": [
        "\n",
        "### 2. Decision Tree Fundamentals (25 points)\n",
        "1. **Gini Index** (5 points)  \n",
        "   Recall the Gini index for a region $R_i$:  \n",
        "   $$\n",
        "   \\operatorname{Gini}(i) \\;=\\; 1 \\;-\\; \\sum_{k} p(k \\mid R_i)^2 \\,.\n",
        "   $$  \n",
        "   - **(3 pts)** Under what scenario would the Gini index achieve its minimum value (0)? What does this indicate about the distribution of classes in region $R_i$?\n",
        "   - **(2 pts)** Under what scenario would the Gini index achieve its maximum value? What would this maximum value be for a problem with $m$ classes, and what does it tell us about the purity of the region?\n",
        "\n",
        "2. **Entropy, KL Divergence, and Decision Trees** (12 points)  \n",
        "   Consider a random forest model predicting a patient's age range (Y) based on various health metrics (X) such as blood pressure, cholesterol levels, and BMI.\n",
        "   \n",
        "   Let X be the set of features and Y be the target variable (age range). The entropy of Y is defined as:\n",
        "   $$\n",
        "   H(Y) = \\sum_{y} p(y) \\, \\log_2 \\!\\Bigl(\\frac{1}{p(y)}\\Bigr),\n",
        "   $$  \n",
        "   where p(y) is the probability of a patient falling into age range y.\n",
        "   \n",
        "   The conditional entropy of Y given X is defined as:\n",
        "   $$\n",
        "   H(Y|X) = \\sum_{x} p(x) H(Y|X=x) = \\sum_{x} p(x) \\sum_{y} p(y|x) \\, \\log_2 \\!\\Bigl(\\frac{1}{p(y|x)}\\Bigr)\n",
        "   $$\n",
        "   \n",
        "   The information gain used in decision trees is defined as:\n",
        "   $$\n",
        "   IG(Y,X) = H(Y) - H(Y|X)\n",
        "   $$\n",
        "   \n",
        "   1. **(4 pts)** Let the *KL-divergence* between two distributions $p$ and $q$ be:  \n",
        "      $$\n",
        "      \\mathrm{KL}(p \\| q) = \\sum_{x} p(x) \\log_2\\Bigl(\\frac{p(x)}{q(x)}\\Bigr),\n",
        "      $$  \n",
        "      Prove that the mutual information $I(Y;X) = H(Y) - H(Y|X)$ (which is the same as information gain) can be written as:  \n",
        "      $$\n",
        "      I(Y; X) = \\mathrm{KL}\\bigl(p(x,y) \\bigl\\| p(x)p(y)\\bigr).\n",
        "      $$\n",
        "      \n",
        "   2. **(4 pts)** In the context of our patient age prediction problem, explain what it means when the KL divergence between the joint distribution p(x,y) and the product of marginals p(x)p(y) is large. How does this relate to feature importance in decision trees?\n",
        "   \n",
        "   3. **(4 pts)** If a feature X provides no information about the target Y, what would be the value of H(Y|X)? What would be the information gain in this case, and what does this mean for decision tree splitting?\n",
        "\n",
        "3. **Constructing an Optimal Decision Tree** (8 points)  \n",
        "\n",
        "  You are given two 2D binary classification datasets, where each point belongs to either class 0 (denoted by ⭘) or class 1 (denoted by ×), as shown in the figure below.\n",
        "\n",
        "  - The **left plot** shows a dataset with 4 data points: two from each class.\n",
        "  - The **right plot** shows a dataset with a larger number of points, where class 1 points (×) form a dense cluster in the center, surrounded by class 0 points (⭘).\n",
        "\n",
        "  #### **Your Task:**\n",
        "\n",
        "  (1) **Decision Tree Construction (4 pts)**\n",
        "\n",
        "  For **each dataset**, describe *verbally* a binary decision tree that achieves **100% classification accuracy**. Your description should include:\n",
        "\n",
        "  - (i) The **feature** used and the **threshold** value at each split (i.e., the decision rule for that node),\n",
        "  - (ii) The **predicted class** in each resulting region,\n",
        "  - (iii) The **depth level** of each split (start with depth 0 at the root).\n",
        "\n",
        "  You **do not** need to draw the decision tree or illustrate the decision regions—focus on a clear verbal description of the logic and structure.\n",
        "\n",
        "  (2) **Tree Depth (3 pts)**  \n",
        "  Report the **depth** of each of your decision trees. Recall that the depth of a tree is defined as the length of the **longest path from the root to a leaf node**, where the root is at depth 0.\n",
        "\n",
        "  (3) **Discussion (1 pt)**  \n",
        "  Briefly discuss whether fully grown decision trees are prone to overfitting, and suggest one or two practical strategies for preventing overfitting in decision tree models.\n",
        "\n",
        "  ![Decision Tree Datasets](https://drive.google.com/uc?id=1bzI5--UGsXEFJU17z3odALx0DS1TA_xn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85618744",
      "metadata": {
        "id": "85618744"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7825bd",
      "metadata": {
        "id": "8b7825bd"
      },
      "source": [
        "### 3. Practical Considerations & Discussion (10 points)\n",
        "1. **(5 pts)** For a highly skewed dataset (e.g., some features have very large numeric values compared to others), explain how you might preprocess or normalize your data before applying $k$-Nearest Neighbors or Decision Trees.  \n",
        "2. **(5 pts)** Are decision trees robust to outliers? Discuss briefly why or why not, and provide one strategy to mitigate the influence of outliers when training trees."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff20c44e",
      "metadata": {
        "id": "ff20c44e"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Multi-Layer Perceptron (MLP) Calculation (10 points)\n",
        "\n",
        "![MLP Diagram](https://drive.google.com/uc?id=1i4le3s3-Q5wSid_W-XCKUgB-BQFm39jK)\n",
        "\n",
        "Consider a 2-layer MLP with the following architecture for binary classification:\n",
        "\n",
        "The network uses the following parameters:\n",
        "- Hidden layer weights: W₁ = [[0.2, 0.5, -0.1], [0.1, -0.3, 0.4]]\n",
        "- Hidden layer biases: b₁ = [0.1, -0.2]\n",
        "- Output layer weights: W₂ = [0.6, 0.3]\n",
        "- Output layer bias: b₂ = -0.1\n",
        "- Activation function for hidden layer: ReLU(x) = max(0, x)\n",
        "- Activation function for output layer: sigmoid(x) = 1/(1+exp(-x))\n",
        "\n",
        "Given an input vector x = [1, 2, 0.5]:\n",
        "\n",
        "1. **(4 pts)** Calculate the values of the hidden layer neurons h₁ and h₂ after applying the ReLU activation.\n",
        "2. **(3 pts)** Calculate the value of the output neuron y after applying the sigmoid activation.\n",
        "3. **(3 pts)** If this is a binary classification problem, what class would this input be assigned to using a threshold of 0.5? Explain why."
      ],
      "metadata": {
        "id": "5GLoI2uc6uy4"
      },
      "id": "5GLoI2uc6uy4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[TODO: Write your responses here. ]**"
      ],
      "metadata": {
        "id": "3VoSzWBj68zB"
      },
      "id": "3VoSzWBj68zB"
    },
    {
      "cell_type": "markdown",
      "id": "16b13c9f",
      "metadata": {
        "id": "16b13c9f"
      },
      "source": [
        "# Part 2: Coding (50 points)\n",
        "\n",
        "**Important**: The coding problems are designed to run as Jupyter notebooks. Throughout the provided starter notebook, you will find **TODO** blocks to guide your implementation. Make sure to fill in these code blocks. **Do not** remove or rename the provided function signatures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd58bcb",
      "metadata": {
        "id": "0dd58bcb"
      },
      "source": [
        "\n",
        "### 2.1.1 Toy Example: 2D Points\n",
        "Let us revisit the two-class setup where:\n",
        "- **Class 1 (Blue points):** $(1,3), (3,6), (4,7)$\n",
        "- **Class 2 (Red points):** $(4,3), (5,5), (2,1)$\n",
        "\n",
        "1. **(5 pts)** **1-NN Decision Boundary**  \n",
        "   - **TODO**: Write Python code to:\n",
        "     1. Store the above data points and labels.\n",
        "     2. Define a function `knn_predict(x, data, labels, k=1)` that returns the predicted class using 1-NN.\n",
        "     3. Plot the points with different colors for each class.\n",
        "     4. Generate a decision boundary mesh (e.g., use `np.meshgrid`) and color each cell according to the 1-NN prediction.  \n",
        "   - Provide a final **visualization** of the boundary in your notebook.\n",
        "\n",
        "2. **(5 pts)** **3-NN Decision Boundary**  \n",
        "   - **TODO**: Reuse your code from above but change $k=3$.  \n",
        "   - Plot and interpret any noticeable differences compared to the $k=1$ boundary.\n",
        "\n",
        "3. **(5 pts)** **Data-Scaling Concern**  \n",
        "   Suppose the blue points become $(1,30), (3,60), (4,70)$, while the red points become $(4,30), (5,50), (2,10)$.  \n",
        "   - **TODO**: Plot and show the **1-NN** decision boundary with these new points.  \n",
        "   - Briefly explain the potential issue here and how you might address it (e.g., normalization).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c17bde0",
      "metadata": {
        "id": "4c17bde0"
      },
      "outputs": [],
      "source": [
        "# TODO: Provide and execute your code implementations here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **(5 pts)** **Choosing $k$ for Larger Datasets**  \n",
        "   - **Write-up in Markdown**: If you have 1000 samples, describe a procedure to find the “optimal” $k$ (e.g., cross-validation).  "
      ],
      "metadata": {
        "id": "SL_6GAAJVjQ9"
      },
      "id": "SL_6GAAJVjQ9"
    },
    {
      "cell_type": "markdown",
      "id": "1950cd90",
      "metadata": {
        "id": "1950cd90"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940737fe",
      "metadata": {
        "id": "940737fe"
      },
      "source": [
        "## 2.2 KNN Regression (10 points)\n",
        "\n",
        "We now explore *KNN Regression* on a small set of 2D points:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&(2,4) \\to 2, \\quad (3,5) \\to 5, \\quad (5,7) \\to 3, \\quad (3,2) \\to 6, \\quad (6,6) \\to 2,\\\\\n",
        "&(8,8) \\to 8, \\quad (4,5) \\to 3, \\quad (2,8) \\to 5, \\quad (7,2) \\to 7, \\quad (9,9) \\to 11.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Assume these are $(x, y)$-coordinates with some *target* (regression) label on the right.\n",
        "\n",
        "1. **(2.5 pts)** Using $k=1$, predict the target for a new point $x'=(10,9)$.  \n",
        "2. **(2.5 pts)** Using $k=3$, predict the target for the same $x'$.  \n",
        "3. **(2.5 pts)** Using $k=10$, predict the target for $x'$.  \n",
        "4. **(2.5 pts)** Over what range of $k$ can you choose? If the true label of $x'$ were 7.5, which $k$ would have given the best prediction?\n",
        "\n",
        "**TODO**:  \n",
        "- Write code to implement a simple `knn_regression_predict(x, data, labels, k)` function.  \n",
        "- Print out the predictions for $k=1, 3, 10$.  \n",
        "- You may hard-code the data points or store them in a NumPy array.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b128ffc4",
      "metadata": {
        "id": "b128ffc4"
      },
      "outputs": [],
      "source": [
        "# TODO: Provide and execute your code implementations here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782a336c",
      "metadata": {
        "id": "782a336c"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0f72c2b",
      "metadata": {
        "id": "b0f72c2b"
      },
      "source": [
        "\n",
        "## 2.3 Decision Tree Exercises (20 points)\n",
        "\n",
        "We will explore decision trees using **scikit-learn** on a real dataset (e.g., the Iris dataset).\n",
        "\n",
        "1. **(5 pts)** **Data Loading & Preprocessing**  \n",
        "   **TODO**:  \n",
        "   - Load the [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) using `sklearn.datasets.load_iris()`.  \n",
        "   - Split it into train/test subsets (e.g., 80% train, 20% test).  \n",
        "\n",
        "2. **(5 pts)** **Training a Decision Tree**  \n",
        "   **TODO**:  \n",
        "   - Train a `DecisionTreeClassifier` from `sklearn.tree` on the training set.  \n",
        "   - Report the training and test accuracies.\n",
        "\n",
        "3. **(5 pts)** **Visualizing the Tree**  \n",
        "   **TODO**:  \n",
        "   - Use `sklearn.tree.plot_tree` to visualize the fitted decision tree.  \n",
        "   - Interpret the first split: which feature was used, and what was the threshold?\n",
        "\n",
        "4. **(5 pts)** **Tree Depth & Pruning**  \n",
        "   **TODO**:  \n",
        "   - Print out the depth of the trained tree.  \n",
        "   - Retrain the tree using a `max_depth` of 2 or 3. Compare the test accuracy with the unpruned tree.  \n",
        "   - Briefly comment on how pruning affects performance and overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aefb540",
      "metadata": {
        "id": "3aefb540"
      },
      "outputs": [],
      "source": [
        "# TODO: Provide and execute your code implementations here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "997e6b4a",
      "metadata": {
        "id": "997e6b4a"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}