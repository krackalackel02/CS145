{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qUzaaWkb0J3"
      },
      "source": [
        "# CS145 Introduction to Data Mining - Assignment 3\n",
        "## Deadline: 11:59PM, May 5, 2025\n",
        "\n",
        "## Instructions\n",
        "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
        "\n",
        "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
        "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
        "\n",
        "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
        "\n",
        "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as x0, x^1, or R x Q for equations.\n",
        "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
        "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
        "\n",
        "### Submission Requirements\n",
        "\n",
        "* Submit your solutions in .ipynb format through GradeScope in BruinLearn.\n",
        "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t\\leq24)e^{-(\\ln(2)/12)t}$.\n",
        "\n",
        "### Collaboration and Integrity\n",
        "\n",
        "* High level discussions are allowed and encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
        "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOMabQY3b0J5"
      },
      "source": [
        "## Part 1: Write-Up Questions\n",
        "\n",
        "### 1. Neural Network Derivatives (12 points total)\n",
        "\n",
        "\n",
        "In this problem, you will analyze the effect of ReLU activation on weight gradients and derive gradients for a softmax cross-entropy loss. **Answer each sub-question carefully with full justification.**\n",
        "\n",
        "#### (a) ReLU Derivatives (6 points)\n",
        "\n",
        "> Definition.\n",
        ">\n",
        "> $$\n",
        "> \\operatorname{ReLU}(x)=\\max (0, x), \\quad \\operatorname{ReLU}^{\\prime}(x)= \\begin{cases}1, & x>0 \\\\ 0, & x \\leq 0\\end{cases}\n",
        "> $$\n",
        ">\n",
        ">\n",
        "> Hint. When a unit's input is non-positive, its ReLU output is zero and \"turns off\" all gradient flow through that unit.\n",
        ">\n",
        "![image-20250420170015320](https://drive.google.com/uc?id=1X0Est80T0gm1N2VrYq8MzmfdoxcdtAjR)\n",
        "\n",
        "Consider a neural network with the following structure:\n",
        "- **Input layer**: $ x_1, x_2 $\n",
        "- **Hidden layer 1**: $ h_3, h_4 $ with ReLU activation\n",
        "- **Hidden layer 2**: $ h_1, h_2 $ with ReLU activation\n",
        "- **Output layer**: $\\hat{y}$\n",
        "\n",
        "The weights $w_1, w_2, \\ldots, w_5$ connect individual units as shown in a diagram, and we aim to minimize a loss function $L$ depending only on the output $\\hat{y}$. Suppose one ReLU unit $h_1$ is “inactive” (its input is negative, so its output is 0).  \n",
        "\n",
        "1. Which partial derivatives\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial w_1}, \\quad \\frac{\\partial L}{\\partial w_2}, \\quad \\frac{\\partial L}{\\partial w_3}\n",
        "   $$\n",
        "   are guaranteed to be zero?  \n",
        "2. **Justify** your answer by explaining how the ReLU activation cuts off gradients for inactive units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ9JUqd6b0J5"
      },
      "source": [
        "### **1(a) ReLU Derivatives**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Which partial derivatives are guaranteed to be zero?**\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial w_1} = 0$  \n",
        "- $\\frac{\\partial L}{\\partial w_2} = 0$  \n",
        "- $\\frac{\\partial L}{\\partial w_3}$ is **not necessarily** zero\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Justification**\n",
        "\n",
        "Since ReLU(x) = $\\max(0, x)$, its derivative is:\n",
        "\n",
        "$$\n",
        "\\text{ReLU}'(x) = \n",
        "\\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "0 & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "We’re told that hidden unit $h_1$ is inactive, meaning its input is $\\leq 0$, so:\n",
        "\n",
        "$$\n",
        "h_1 = 0 \\quad \\text{and} \\quad \\frac{\\partial h_1}{\\partial (\\cdot)} = 0\n",
        "$$\n",
        "\n",
        "This means the gradient $\\frac{\\partial L}{\\partial h_1}$ is blocked — no gradients flow through $h_1$ to earlier layers.\n",
        "\n",
        "So:\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1} = 0$\n",
        "- $\\frac{\\partial L}{\\partial w_2}$ also involves $h_1$, so it’s 0 too.\n",
        "\n",
        "However, $\\frac{\\partial L}{\\partial w_3}$ is connected to $h_3$ via $x_1$. We’re not told that $h_3$ is inactive, and it might affect $\\hat{y}$ through another active path (e.g., $h_2$), so the gradient might still flow.\n",
        "\n",
        "In short: if a ReLU unit is inactive, the gradient flow through it stops. Any weight that only influences the output through that unit gets zero gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plw18jZBb0J5"
      },
      "source": [
        "#### (b) Cross-Entropy Loss: Chain Rule Derivation (6 points)\n",
        "\n",
        "> We now replace the softmax/matrix formulation with a single‑neuron network using sigmoid + BCE.\n",
        "\n",
        "Consider a neural network with:\n",
        "- Inputs: $x_1, x_2$\n",
        "- Output neuron $z=w_1 x_1+w_2 x_2+b$\n",
        "- Activation: $\\hat{y}=\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
        "- True label: $y \\in\\{0,1\\}$\n",
        "- Loss:\n",
        "\n",
        "$$\n",
        "L=-[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})] \\quad \\text { (binary cross-entropy) }\n",
        "$$\n",
        "\n",
        "\n",
        "**Task.** Derive expressions for\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1}, \\quad \\frac{\\partial L}{\\partial w_2}, \\quad \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "by unfolding the chain rule through the sigmoid activation and the BCE loss.\n",
        "Be sure to show each step:\n",
        "1. $\\partial L / \\partial \\hat{y}$\n",
        "2. $\\partial \\hat{y} / \\partial z$\n",
        "3. $\\partial z / \\partial w_i$ and $\\partial z / \\partial b$\n",
        "4. Combine to get $\\partial L / \\partial w_i, \\partial L / \\partial b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReYp4ULb0J5"
      },
      "source": [
        "### **1(b) Cross-Entropy Loss: Chain Rule Derivation**\n",
        "---\n",
        "\n",
        "#### **Compute** $\\frac{\\partial L}{\\partial \\hat{y}}$\n",
        "\n",
        "Using the chain rule on the loss:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Compute** $\\frac{\\partial \\hat{y}}{\\partial z}$\n",
        "\n",
        "Since $\\hat{y} = \\sigma(z)$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Compute** $\\frac{\\partial z}{\\partial w_i}$ and $\\frac{\\partial z}{\\partial b}$\n",
        "\n",
        "From $z = w_1x_1 + w_2x_2 + b$:\n",
        "\n",
        "- $\\frac{\\partial z}{\\partial w_1} = x_1$\n",
        "- $\\frac{\\partial z}{\\partial w_2} = x_2$\n",
        "- $\\frac{\\partial z}{\\partial b} = 1$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Combine Using the Chain Rule**\n",
        "\n",
        "Now apply the full chain rule:\n",
        "\n",
        "**For $w_1$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_1}\n",
        "= \\left( -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\right) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x_1\n",
        "$$\n",
        "\n",
        "Simplifying this expression (using the identity that appears often in practice), we get:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = (\\hat{y} - y) \\cdot x_1\n",
        "$$\n",
        "\n",
        "**Similarly:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_2} = (\\hat{y} - y) \\cdot x_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = (\\hat{y} - y) \\cdot 1 = \\hat{y} - y\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Final Answer**\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial w_1} = (\\hat{y} - y) \\cdot x_1$\n",
        "- $\\frac{\\partial L}{\\partial w_2} = (\\hat{y} - y) \\cdot x_2$\n",
        "- $\\frac{\\partial L}{\\partial b} = \\hat{y} - y$\n",
        "\n",
        "These gradients are used to update the weights during training in logistic regression or a single-neuron binary classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlYSUq-b0J6"
      },
      "source": [
        "### 2. Two-Layer MLP for XOR (6 points)\n",
        "\n",
        "We know a single-layer perceptron cannot represent the XOR function. A **two-layer network** with an appropriate choice of weights and biases can solve it.  \n",
        "\n",
        "1. **Construct** such a two-layer MLP (with a single hidden layer) that outputs 1 for XOR=1 and 0 for XOR=0, given inputs $\\{(x_1, x_2)\\mid x_1,x_2 \\in \\{0,1\\}\\}$.  \n",
        "   - Specify your network’s architecture (size of hidden layer, activation function, final layer).\n",
        "   - Provide **explicit** weight and bias values.  \n",
        "2. **Demonstrate** that for all $(x_1,x_2)$ in $\\{0,1\\}\\times\\{0,1\\}$, the final output is correct (0 or 1) for XOR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GEWiia0b0J6"
      },
      "source": [
        "### 2. Two-Layer MLP for XOR (6 points)\n",
        "\n",
        "#### 1. Network Architecture\n",
        "\n",
        "We design a 2-layer MLP that computes the XOR function using:\n",
        "\n",
        "- **Input:** $x_1, x_2$\n",
        "- **Hidden layer:** 2 neurons, ReLU activation\n",
        "- **Output layer:** 1 neuron, ReLU (or identity) activation\n",
        "\n",
        "#### Weights and Biases\n",
        "\n",
        "Define:\n",
        "\n",
        "$$\n",
        "W^{(1)} = \n",
        "\\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}, \\quad\n",
        "b^{(1)} = \n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "-1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{(2)} = \n",
        "\\begin{bmatrix}\n",
        "1 & -2\n",
        "\\end{bmatrix}, \\quad\n",
        "b^{(2)} = 0\n",
        "$$\n",
        "\n",
        "The network computes:\n",
        "\n",
        "$$\n",
        "z_1 = W^{(1)} x + b^{(1)} \\\\\n",
        "h = \\text{ReLU}(z_1) \\\\\n",
        "\\hat{y} = \\text{ReLU}(W^{(2)} h + b^{(2)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Verifying XOR Outputs\n",
        "\n",
        "---\n",
        "\n",
        "**Case 1: $(x_1, x_2) = (0, 0)$**\n",
        "\n",
        "$$\n",
        "z_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \n",
        "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \n",
        "+ \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n",
        "= \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h = \\text{ReLU}(z_1) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{ReLU}\\left( \\begin{bmatrix} 1 & -2 \\end{bmatrix}\n",
        "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + 0 \\right) \n",
        "= \\text{ReLU}(0) = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Case 2: $(x_1, x_2) = (1, 0)$**\n",
        "\n",
        "$$\n",
        "z_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \n",
        "\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \n",
        "+ \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n",
        "= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h = \\text{ReLU}(z_1) = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{ReLU}\\left( \\begin{bmatrix} 1 & -2 \\end{bmatrix}\n",
        "\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 0 \\right) \n",
        "= \\text{ReLU}(1) = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Case 3: $(x_1, x_2) = (0, 1)$**\n",
        "\n",
        "$$\n",
        "z_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \n",
        "\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \n",
        "+ \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n",
        "= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h = \\text{ReLU}(z_1) = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{ReLU}\\left( \\begin{bmatrix} 1 & -2 \\end{bmatrix}\n",
        "\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 0 \\right) \n",
        "= \\text{ReLU}(1) = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**Case 4: $(x_1, x_2) = (1, 1)$**\n",
        "\n",
        "$$\n",
        "z_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \n",
        "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \n",
        "+ \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}\n",
        "= \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h = \\text{ReLU}(z_1) = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{ReLU}\\left( \\begin{bmatrix} 1 & -2 \\end{bmatrix}\n",
        "\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} + 0 \\right)\n",
        "= \\text{ReLU}(2 - 2) = \\text{ReLU}(0) = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Thus, the network correctly computes the XOR function for all 4 possible binary input combinations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyn-Krj9b0J6"
      },
      "source": [
        "### 3. $K$-Means Clustering (8 points)\n",
        "\n",
        "![image-20250420170622686](https://drive.google.com/uc?id=1SUOHENOHHkgQvRzSI3FIvaZF37c4wPuQ)\n",
        "\n",
        "Recall the following data points in $\\mathbb{R}^2$:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "5.9 & 3.2 \\\\\n",
        "4.6 & 2.9 \\\\\n",
        "6.2 & 2.8 \\\\\n",
        "4.7 & 3.2 \\\\\n",
        "5.5 & 4.2 \\\\\n",
        "5.0 & 3.0 \\\\\n",
        "4.9 & 3.1 \\\\\n",
        "6.7 & 3.1 \\\\\n",
        "5.1 & 3.8 \\\\\n",
        "6.0 & 3.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We apply $K$-Means with $K=3$ using Euclidean distance, starting from cluster centers $\\boldsymbol{\\mu}_1=(6.2,3.2), \\boldsymbol{\\mu}_2=(6.6,3.7), \\boldsymbol{\\mu}_3=(6.5,3.0)$.\n",
        "\n",
        "1. **(2 pts)** Calculate the center of Cluster 1 after one iteration. Show your intermediate step of assigning points and then compute the updated mean.\n",
        "2. **(2 pts)** Compute the center of Cluster 2 after **two** iterations (round to three decimals).\n",
        "3. **(2 pts)** Find the center of Cluster 3 at convergence (round to three decimals).\n",
        "4. **(2 pts)** How many iterations does it take to converge (no changes in cluster assignments)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA51CAaib0J6"
      },
      "source": [
        "### 3. $K$-Means Clustering (8 points)\n",
        "\n",
        "We are given:\n",
        "\n",
        "- Data matrix $\\mathbf{X}$ of 10 points in $\\mathbb{R}^2$\n",
        "- Initial cluster centers:\n",
        "\n",
        "$$\n",
        "\\mu_1 = (6.2, 3.2), \\quad \\mu_2 = (6.6, 3.7), \\quad \\mu_3 = (6.5, 3.0)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Iteration 1: Distance Table and Assignments\n",
        "\n",
        "| Point             | $\\mu_1$ (6.2, 3.2) | $\\mu_2$ (6.6, 3.7) | $\\mu_3$ (6.5, 3.0) | Assigned |\n",
        "|------------------|-------------------|--------------------|--------------------|----------|\n",
        "| (5.9, 3.2)        | **0.30**          | 0.86               | 0.63               | 1        |\n",
        "| (4.6, 2.9)        | **1.63**          | 2.15               | 1.90               | 1        |\n",
        "| (6.2, 2.8)        | 0.40              | 0.98               | **0.36**           | 3        |\n",
        "| (4.7, 3.2)        | **1.50**          | 1.96               | 1.81               | 1        |\n",
        "| (5.5, 4.2)        | 1.22              | **1.21**           | 1.56               | 2        |\n",
        "| (5.0, 3.0)        | **1.22**          | 1.75               | 1.50               | 1        |\n",
        "| (4.9, 3.1)        | **1.30**          | 1.80               | 1.60               | 1        |\n",
        "| (6.7, 3.1)        | 0.51              | 0.61               | **0.22**           | 3        |\n",
        "| (5.1, 3.8)        | **1.25**          | 1.50               | 1.61               | 1        |\n",
        "| (6.0, 3.0)        | **0.28**          | 0.92               | 0.50               | 1        |\n",
        "\n",
        "**New center for Cluster 1**:\n",
        "\n",
        "$$\n",
        "\\mu_1^{(1)} = \\frac{1}{7} \\sum_{i \\in C_1} x_i = (5.17,\\ 3.17)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Iteration 2: Distance Table and Assignments\n",
        "\n",
        "| Point             | $\\mu_1$ (5.17, 3.17) | $\\mu_2$ (5.5, 4.2) | $\\mu_3$ (6.45, 2.95) | Assigned |\n",
        "|------------------|---------------------|--------------------|----------------------|----------|\n",
        "| (5.9, 3.2)        | **0.73**            | 1.08               | 0.60                 | 3        |\n",
        "| (4.6, 2.9)        | **0.63**            | 1.58               | 1.85                 | 1        |\n",
        "| (6.2, 2.8)        | 1.09                | 1.57               | **0.29**             | 3        |\n",
        "| (4.7, 3.2)        | **0.47**            | 1.28               | 1.77                 | 1        |\n",
        "| (5.5, 4.2)        | 1.08                | **0.00**           | 1.57                 | 2        |\n",
        "| (5.0, 3.0)        | **0.24**            | 1.30               | 1.45                 | 1        |\n",
        "| (4.9, 3.1)        | **0.28**            | 1.25               | 1.56                 | 1        |\n",
        "| (6.7, 3.1)        | 1.53                | 1.63               | **0.29**             | 3        |\n",
        "| (5.1, 3.8)        | 0.63                | **0.57**           | 1.60                 | 2        |\n",
        "| (6.0, 3.0)        | 0.85                | 1.30               | **0.45**             | 3        |\n",
        "\n",
        "**New center for Cluster 2**:\n",
        "\n",
        "$$\n",
        "\\mu_2^{(2)} = \\frac{1}{2} \\left[ (5.1, 3.8) + (5.5, 4.2) \\right] = (5.3,\\ 4.0)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Iteration 3: Distance Table and Final Assignment\n",
        "\n",
        "| Point             | $\\mu_1$ (4.80, 3.05) | $\\mu_2$ (5.3, 4.0) | $\\mu_3$ (6.2, 3.03) | Assigned |\n",
        "|------------------|---------------------|--------------------|---------------------|----------|\n",
        "| (5.9, 3.2)        | 1.11                | 1.00               | **0.35**            | 3        |\n",
        "| (4.6, 2.9)        | **0.25**            | 1.30               | 1.60                | 1        |\n",
        "| (6.2, 2.8)        | 1.42                | 1.50               | **0.23**            | 3        |\n",
        "| (4.7, 3.2)        | **0.18**            | 1.00               | 1.51                | 1        |\n",
        "| (5.5, 4.2)        | 1.35                | **0.28**           | 1.37                | 2        |\n",
        "| (5.0, 3.0)        | **0.21**            | 1.04               | 1.20                | 1        |\n",
        "| (4.9, 3.1)        | **0.11**            | 0.98               | 1.30                | 1        |\n",
        "| (6.7, 3.1)        | 1.90                | 1.66               | **0.51**            | 3        |\n",
        "| (5.1, 3.8)        | 0.81                | **0.28**           | 1.35                | 2        |\n",
        "| (6.0, 3.0)        | 1.20                | 1.22               | **0.20**            | 3        |\n",
        "\n",
        "**Final centers:**\n",
        "\n",
        "- Cluster 1:  \n",
        "  $$\n",
        "  \\mu_1 = \\frac{1}{4} \\left[ (4.6, 2.9) + (4.7, 3.2) + (4.9, 3.1) + (5.0, 3.0) \\right] = (4.8,\\ 3.05)\n",
        "  $$\n",
        "\n",
        "- Cluster 2: $(5.3,\\ 4.0)$  \n",
        "- Cluster 3: $(6.2,\\ 3.025)$  \n",
        "\n",
        "---\n",
        "\n",
        "### Number of Iterations to Converge\n",
        "\n",
        "Convergence occurs when assignments no longer change.\n",
        "\n",
        "**Answer:** $3$ iterations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Iteration 1 ===\n",
            "   mu1 (6.2, 3.2)  mu2 (6.6, 3.7)  mu3 (6.5, 3.0)  Assigned Cluster\n",
            "0            0.30            0.86            0.63                 1\n",
            "1            1.63            2.15            1.90                 1\n",
            "2            0.40            0.98            0.36                 3\n",
            "3            1.50            1.96            1.81                 1\n",
            "4            1.22            1.21            1.56                 2\n",
            "5            1.22            1.75            1.50                 1\n",
            "6            1.30            1.80            1.60                 1\n",
            "7            0.51            0.61            0.22                 3\n",
            "8            1.25            1.50            1.61                 1\n",
            "9            0.28            0.92            0.50                 1\n",
            "\n",
            "=== Iteration 2 ===\n",
            "   mu1 (5.171428571428572, 3.1714285714285713)  mu2 (5.5, 4.2)  \\\n",
            "0                                         0.73            1.08   \n",
            "1                                         0.63            1.58   \n",
            "2                                         1.09            1.57   \n",
            "3                                         0.47            1.28   \n",
            "4                                         1.08            0.00   \n",
            "5                                         0.24            1.30   \n",
            "6                                         0.28            1.25   \n",
            "7                                         1.53            1.63   \n",
            "8                                         0.63            0.57   \n",
            "9                                         0.85            1.30   \n",
            "\n",
            "   mu3 (6.45, 2.95)  Assigned Cluster  \n",
            "0              0.60                 3  \n",
            "1              1.85                 1  \n",
            "2              0.29                 3  \n",
            "3              1.77                 1  \n",
            "4              1.57                 2  \n",
            "5              1.45                 1  \n",
            "6              1.56                 1  \n",
            "7              0.29                 3  \n",
            "8              1.60                 2  \n",
            "9              0.45                 3  \n",
            "\n",
            "=== Iteration 3 ===\n",
            "   mu1 (4.800000000000001, 3.05)  mu2 (5.3, 4.0)  mu3 (6.2, 3.025)  \\\n",
            "0                           1.11            1.00              0.35   \n",
            "1                           0.25            1.30              1.60   \n",
            "2                           1.42            1.50              0.23   \n",
            "3                           0.18            1.00              1.51   \n",
            "4                           1.35            0.28              1.37   \n",
            "5                           0.21            1.04              1.20   \n",
            "6                           0.11            0.98              1.30   \n",
            "7                           1.90            1.66              0.51   \n",
            "8                           0.81            0.28              1.35   \n",
            "9                           1.20            1.22              0.20   \n",
            "\n",
            "   Assigned Cluster  \n",
            "0                 3  \n",
            "1                 1  \n",
            "2                 3  \n",
            "3                 1  \n",
            "4                 2  \n",
            "5                 1  \n",
            "6                 1  \n",
            "7                 3  \n",
            "8                 2  \n",
            "9                 3  \n",
            "\n",
            "Convergence reached.\n",
            "\n",
            "Final Cluster Centers:\n",
            "[[4.8   3.05 ]\n",
            " [5.3   4.   ]\n",
            " [6.2   3.025]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define data points\n",
        "X = np.array([\n",
        "    [5.9, 3.2],\n",
        "    [4.6, 2.9],\n",
        "    [6.2, 2.8],\n",
        "    [4.7, 3.2],\n",
        "    [5.5, 4.2],\n",
        "    [5.0, 3.0],\n",
        "    [4.9, 3.1],\n",
        "    [6.7, 3.1],\n",
        "    [5.1, 3.8],\n",
        "    [6.0, 3.0]\n",
        "])\n",
        "\n",
        "# Initial centers\n",
        "mu = np.array([\n",
        "    [6.2, 3.2],\n",
        "    [6.6, 3.7],\n",
        "    [6.5, 3.0]\n",
        "])\n",
        "\n",
        "iteration = 0\n",
        "prev_assignments = None\n",
        "\n",
        "while True:\n",
        "    iteration += 1\n",
        "    print(f\"\\n=== Iteration {iteration} ===\")\n",
        "    \n",
        "    # Compute distances\n",
        "    distances = cdist(X, mu, metric='euclidean')\n",
        "    \n",
        "    # Assign clusters\n",
        "    assignments = np.argmin(distances, axis=1)\n",
        "    \n",
        "    # Show distance matrix and assignments\n",
        "    df = pd.DataFrame(distances, columns=[f'mu{i+1} {tuple(mu[i])}' for i in range(3)])\n",
        "    df['Assigned Cluster'] = assignments + 1\n",
        "    print(df.round(2))\n",
        "    \n",
        "    # Check for convergence\n",
        "    if np.array_equal(assignments, prev_assignments):\n",
        "        print(\"\\nConvergence reached.\")\n",
        "        break\n",
        "    \n",
        "    # Update centers\n",
        "    new_mu = []\n",
        "    for k in range(3):\n",
        "        cluster_points = X[assignments == k]\n",
        "        if len(cluster_points) > 0:\n",
        "            new_mu.append(cluster_points.mean(axis=0))\n",
        "        else:\n",
        "            new_mu.append(mu[k])  # if empty, retain old center\n",
        "    mu = np.array(new_mu)\n",
        "    \n",
        "    prev_assignments = assignments.copy()\n",
        "\n",
        "# Final cluster centers\n",
        "print(\"\\nFinal Cluster Centers:\")\n",
        "print(mu.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuEyJJk8b0J6"
      },
      "source": [
        "## Part 2: Coding Problems\n",
        "\n",
        "Below is a skeleton of a PyTorch-based notebook. You will fill in the indicated `TODO` blocks. We will focus on:\n",
        "\n",
        "1. **Implementing Dropout** in a fully connected network.\n",
        "2. **Implementing k-Fold Cross-Validation** for hyperparameter selection.\n",
        "\n",
        "### **Scoring Breakdown (20 points total)**\n",
        "\n",
        "1. **Insert Dropout (5 points)**  \n",
        "   *Implementation of `nn.Dropout` in the network initialization and forward pass.*\n",
        "\n",
        "2. **Evaluate Dropout Performance (5 points)**  \n",
        "   *Show how dropout changes training/test performance (e.g., final accuracy). Provide a brief analysis.*\n",
        "\n",
        "3. **Implement k-Fold Cross-Validation (5 points)**  \n",
        "   *Write a procedure that splits the training data into $k$ folds, trains on $k-1$ folds, and validates on the remaining fold.*\n",
        "\n",
        "4. **Hyperparameter Tuning and Final Results (5 points)**  \n",
        "   *Vary hyperparameters (e.g., learning rate, hidden dimension, dropout probability), identify the best setting, and report final test accuracy.*\n",
        "\n",
        "Below is a condensed example for reference. Use (and modify) as needed, and ensure each `TODO` is addressed in your final notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "covuS5U3b0J6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Loading Data...\n",
            "Data loaded.\n",
            "\n",
            "2. Defining Model...\n",
            "Model defined.\n",
            "\n",
            "3. Defining Training and Evaluation Functions...\n",
            "Training and evaluation functions defined.\n"
          ]
        }
      ],
      "source": [
        "# %pip install torch torchvision scikit-learn numpy --user # Ensure libraries are installed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, RandomSampler, Subset # Added Subset\n",
        "from sklearn.model_selection import KFold # Added KFold\n",
        "from itertools import product # Added product\n",
        "\n",
        "# Set a seed for reproducibility (optional but good practice)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "##################################\n",
        "# 1. Data Loading and Transforms #\n",
        "##################################\n",
        "print(\"1. Loading Data...\") # Keep original print\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "# Keep original variable name train_loader for the full dataset initially\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"Data loaded.\") # Keep original print\n",
        "\n",
        "############################################\n",
        "# 2. Model Definition + Dropout (5 points) #\n",
        "############################################\n",
        "print(\"\\n2. Defining Model...\") # Keep original print\n",
        "class SimpleFCNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10, dropout_prob=0.5):\n",
        "        super(SimpleFCNetwork, self).__init__()\n",
        "\n",
        "        # Store hyperparams (optional but good practice)\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob # Store dropout prob\n",
        "\n",
        "        # 2-layer fully-connected\n",
        "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "\n",
        "        # TODO (Insert Dropout layer here) - (3 points)\n",
        "        # --- Implementation Start ---\n",
        "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
        "        # --- Implementation End ---\n",
        "\n",
        "        self.fc2 = nn.Linear(self.hidden_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # TODO (Apply dropout here) - (2 points)\n",
        "        # --- Implementation Start ---\n",
        "        # Dropout is automatically handled by model.train() vs model.eval()\n",
        "        x = self.dropout(x)\n",
        "        # --- Implementation End ---\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "print(\"Model defined.\") # Keep original print\n",
        "\n",
        "##############################\n",
        "# 3. Training and Evaluation #\n",
        "##############################\n",
        "print(\"\\n3. Defining Training and Evaluation Functions...\") # Keep original print\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    \"\"\" Trains the model, keeping original print statement format \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            # Optional: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            # images, labels = images.to(device), labels.to(device)\n",
        "            # model.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        # Maintain original print statement (prints loss every epoch)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    \"\"\" Evaluates the model, same as before \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "             # Optional: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "             # images, labels = images.to(device), labels.to(device)\n",
        "             # model.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "print(\"Training and evaluation functions defined.\") # Keep original print\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Insert Dropout (Implementation)\n",
        "\n",
        "* **Status:** Completed.  \n",
        "* **Details:** The `SimpleFCNetwork` class was modified as required within the original template structure. An `nn.Dropout` layer was instantiated in `__init__` and applied in the `forward` method after the ReLU activation, replacing the `TODO` placeholders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running initial example usage...\n",
            "Epoch [1/10], Loss: 1.7445\n",
            "Epoch [2/10], Loss: 1.1834\n",
            "Epoch [3/10], Loss: 0.9802\n",
            "Epoch [4/10], Loss: 0.8824\n",
            "Epoch [5/10], Loss: 0.8180\n",
            "Epoch [6/10], Loss: 0.7761\n",
            "Epoch [7/10], Loss: 0.7457\n",
            "Epoch [8/10], Loss: 0.7193\n",
            "Epoch [9/10], Loss: 0.6998\n",
            "Epoch [10/10], Loss: 0.6814\n",
            "Test Accuracy from initial example (Dropout=0.5, SGD): 78.25%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage (for demonstration; you will do more thorough experiments):\n",
        "# --- Keep original example ---\n",
        "print(\"\\nRunning initial example usage...\")\n",
        "model = SimpleFCNetwork(dropout_prob=0.5) # Uses default hidden_dim=128\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3) # Original uses SGD\n",
        "\n",
        "# Note: This trains a model but its results are not directly used later.\n",
        "# The dropout evaluation and CV parts will train their own models.\n",
        "model = train_model(model, train_loader, criterion, optimizer, epochs=10) # Original uses 10 epochs\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy from initial example (Dropout=0.5, SGD): {test_accuracy:.2f}%\") # Modified print slightly for clarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Evaluating Dropout Performance (Using Adam Optimizer)...\n",
            "Training model WITHOUT dropout (p=0.0)...\n",
            "Epoch [1/10], Loss: 0.4971\n",
            "Epoch [2/10], Loss: 0.3766\n",
            "Epoch [3/10], Loss: 0.3398\n",
            "Epoch [4/10], Loss: 0.3157\n",
            "Epoch [5/10], Loss: 0.2991\n",
            "Epoch [6/10], Loss: 0.2845\n",
            "Epoch [7/10], Loss: 0.2698\n",
            "Epoch [8/10], Loss: 0.2591\n",
            "Epoch [9/10], Loss: 0.2494\n",
            "Epoch [10/10], Loss: 0.2396\n",
            "Test Accuracy WITHOUT Dropout: 88.30%\n",
            "\n",
            "Training model WITH dropout (p=0.5)...\n",
            "Epoch [1/10], Loss: 0.6170\n",
            "Epoch [2/10], Loss: 0.4804\n",
            "Epoch [3/10], Loss: 0.4532\n",
            "Epoch [4/10], Loss: 0.4364\n",
            "Epoch [5/10], Loss: 0.4172\n",
            "Epoch [6/10], Loss: 0.4086\n",
            "Epoch [7/10], Loss: 0.3998\n",
            "Epoch [8/10], Loss: 0.3904\n",
            "Epoch [9/10], Loss: 0.3841\n",
            "Epoch [10/10], Loss: 0.3791\n",
            "Test Accuracy WITH Dropout (p=0.5): 86.94%\n",
            "\n",
            "Dropout Performance Analysis:\n",
            "Comparing test accuracies (Adam, 10 epochs): No Dropout=88.30%, Dropout(p=0.5)=86.94%.\n",
            "Dropout slightly decreased test accuracy in this run.\n"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "# 4. Evaluate Dropout Performance (Added) #\n",
        "###########################################\n",
        "print(\"\\n4. Evaluating Dropout Performance (Using Adam Optimizer)...\")\n",
        "\n",
        "# --- Train and Evaluate WITHOUT Dropout ---\n",
        "print(\"Training model WITHOUT dropout (p=0.0)...\")\n",
        "model_no_dropout = SimpleFCNetwork(hidden_dim=128, dropout_prob=0.0) # Fixed hidden_dim\n",
        "criterion_comp = nn.CrossEntropyLoss()\n",
        "optimizer_no_dropout = optim.Adam(model_no_dropout.parameters(), lr=1e-3) # Use Adam here\n",
        "\n",
        "train_model(model_no_dropout, train_loader, criterion_comp, optimizer_no_dropout, epochs=10) # Use 10 epochs for comparison\n",
        "test_accuracy_no_dropout = evaluate_model(model_no_dropout, test_loader)\n",
        "print(f\"Test Accuracy WITHOUT Dropout: {test_accuracy_no_dropout:.2f}%\")\n",
        "\n",
        "# --- Train and Evaluate WITH Dropout ---\n",
        "print(\"\\nTraining model WITH dropout (p=0.5)...\")\n",
        "model_with_dropout = SimpleFCNetwork(hidden_dim=128, dropout_prob=0.5) # Fixed hidden_dim\n",
        "optimizer_with_dropout = optim.Adam(model_with_dropout.parameters(), lr=1e-3) # Use Adam here\n",
        "\n",
        "train_model(model_with_dropout, train_loader, criterion_comp, optimizer_with_dropout, epochs=10) # Use 10 epochs for comparison\n",
        "test_accuracy_with_dropout = evaluate_model(model_with_dropout, test_loader)\n",
        "print(f\"Test Accuracy WITH Dropout (p=0.5): {test_accuracy_with_dropout:.2f}%\")\n",
        "\n",
        "# --- Analysis printout ---\n",
        "print(\"\\nDropout Performance Analysis:\")\n",
        "print(f\"Comparing test accuracies (Adam, 10 epochs): No Dropout={test_accuracy_no_dropout:.2f}%, Dropout(p=0.5)={test_accuracy_with_dropout:.2f}%.\")\n",
        "if test_accuracy_with_dropout > test_accuracy_no_dropout:\n",
        "    print(\"Dropout improved test accuracy in this run.\")\n",
        "elif test_accuracy_with_dropout < test_accuracy_no_dropout:\n",
        "    print(\"Dropout slightly decreased test accuracy in this run.\")\n",
        "else:\n",
        "    print(\"Dropout had minimal impact in this run.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluate Dropout Performance\n",
        "\n",
        "* **Objective:** To compare the test performance of the network with and without dropout enabled, using fixed hyperparameters (hidden_dim=128, lr=1e-3 with Adam optimizer, 10 epochs).\n",
        "* **Results:**\n",
        "    * Test Accuracy **WITHOUT** Dropout (p=0.0): `88.30%`\n",
        "    * Test Accuracy **WITH** Dropout (p=0.5): `86.94%`\n",
        "* **Analysis:** The dedicated comparison in section 4 showed that using dropout (p=0.5) slightly *decreased* test accuracy compared to no dropout for this specific 10-epoch run with the Adam optimizer. This suggests the model might not have been overfitting significantly in that short run, or p=0.5 was too high. The initial example run (using SGD and p=0.5) achieved a different accuracy (`78.25%`), highlighting sensitivity to the optimizer and training details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5. Implementing k-Fold Cross-Validation and Hyperparameter Tuning...\n",
            "\n",
            "Starting 3-Fold Cross-Validation with 8 parameter combinations.\n",
            "Training for 5 epochs per fold.\n",
            "\n",
            "Testing Combination 1/8: {'lr': 0.001, 'hidden_dim': 128, 'dropout_prob': 0.3}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.5913\n",
            "Epoch [2/5], Loss: 0.4516\n",
            "Epoch [3/5], Loss: 0.4097\n",
            "Epoch [4/5], Loss: 0.3894\n",
            "Epoch [5/5], Loss: 0.3681\n",
            "    Fold 1 Validation Accuracy: 86.92%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.5890\n",
            "Epoch [2/5], Loss: 0.4517\n",
            "Epoch [3/5], Loss: 0.4110\n",
            "Epoch [4/5], Loss: 0.3895\n",
            "Epoch [5/5], Loss: 0.3749\n",
            "    Fold 2 Validation Accuracy: 86.59%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.5893\n",
            "Epoch [2/5], Loss: 0.4499\n",
            "Epoch [3/5], Loss: 0.4100\n",
            "Epoch [4/5], Loss: 0.3883\n",
            "Epoch [5/5], Loss: 0.3731\n",
            "    Fold 3 Validation Accuracy: 87.08%\n",
            "  Params: {'lr': 0.001, 'hidden_dim': 128, 'dropout_prob': 0.3}, Avg Validation Accuracy across 3 folds: 86.86%\n",
            "  >> New best parameters found!\n",
            "\n",
            "Testing Combination 2/8: {'lr': 0.001, 'hidden_dim': 128, 'dropout_prob': 0.5}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.6632\n",
            "Epoch [2/5], Loss: 0.5088\n",
            "Epoch [3/5], Loss: 0.4713\n",
            "Epoch [4/5], Loss: 0.4450\n",
            "Epoch [5/5], Loss: 0.4362\n",
            "    Fold 1 Validation Accuracy: 86.73%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.6600\n",
            "Epoch [2/5], Loss: 0.5087\n",
            "Epoch [3/5], Loss: 0.4690\n",
            "Epoch [4/5], Loss: 0.4493\n",
            "Epoch [5/5], Loss: 0.4379\n",
            "    Fold 2 Validation Accuracy: 86.42%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.6475\n",
            "Epoch [2/5], Loss: 0.4951\n",
            "Epoch [3/5], Loss: 0.4627\n",
            "Epoch [4/5], Loss: 0.4428\n",
            "Epoch [5/5], Loss: 0.4240\n",
            "    Fold 3 Validation Accuracy: 85.90%\n",
            "  Params: {'lr': 0.001, 'hidden_dim': 128, 'dropout_prob': 0.5}, Avg Validation Accuracy across 3 folds: 86.35%\n",
            "\n",
            "Testing Combination 3/8: {'lr': 0.001, 'hidden_dim': 256, 'dropout_prob': 0.3}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.5668\n",
            "Epoch [2/5], Loss: 0.4377\n",
            "Epoch [3/5], Loss: 0.3987\n",
            "Epoch [4/5], Loss: 0.3762\n",
            "Epoch [5/5], Loss: 0.3580\n",
            "    Fold 1 Validation Accuracy: 87.02%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.5639\n",
            "Epoch [2/5], Loss: 0.4348\n",
            "Epoch [3/5], Loss: 0.3979\n",
            "Epoch [4/5], Loss: 0.3726\n",
            "Epoch [5/5], Loss: 0.3552\n",
            "    Fold 2 Validation Accuracy: 87.48%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.5600\n",
            "Epoch [2/5], Loss: 0.4305\n",
            "Epoch [3/5], Loss: 0.3968\n",
            "Epoch [4/5], Loss: 0.3710\n",
            "Epoch [5/5], Loss: 0.3601\n",
            "    Fold 3 Validation Accuracy: 86.55%\n",
            "  Params: {'lr': 0.001, 'hidden_dim': 256, 'dropout_prob': 0.3}, Avg Validation Accuracy across 3 folds: 87.02%\n",
            "  >> New best parameters found!\n",
            "\n",
            "Testing Combination 4/8: {'lr': 0.001, 'hidden_dim': 256, 'dropout_prob': 0.5}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.6058\n",
            "Epoch [2/5], Loss: 0.4750\n",
            "Epoch [3/5], Loss: 0.4429\n",
            "Epoch [4/5], Loss: 0.4209\n",
            "Epoch [5/5], Loss: 0.4048\n",
            "    Fold 1 Validation Accuracy: 86.53%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.6109\n",
            "Epoch [2/5], Loss: 0.4801\n",
            "Epoch [3/5], Loss: 0.4441\n",
            "Epoch [4/5], Loss: 0.4190\n",
            "Epoch [5/5], Loss: 0.4094\n",
            "    Fold 2 Validation Accuracy: 86.94%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.6086\n",
            "Epoch [2/5], Loss: 0.4744\n",
            "Epoch [3/5], Loss: 0.4415\n",
            "Epoch [4/5], Loss: 0.4153\n",
            "Epoch [5/5], Loss: 0.4071\n",
            "    Fold 3 Validation Accuracy: 86.40%\n",
            "  Params: {'lr': 0.001, 'hidden_dim': 256, 'dropout_prob': 0.5}, Avg Validation Accuracy across 3 folds: 86.62%\n",
            "\n",
            "Testing Combination 5/8: {'lr': 0.0005, 'hidden_dim': 128, 'dropout_prob': 0.3}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.6396\n",
            "Epoch [2/5], Loss: 0.4650\n",
            "Epoch [3/5], Loss: 0.4252\n",
            "Epoch [4/5], Loss: 0.3968\n",
            "Epoch [5/5], Loss: 0.3785\n",
            "    Fold 1 Validation Accuracy: 87.24%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.6424\n",
            "Epoch [2/5], Loss: 0.4650\n",
            "Epoch [3/5], Loss: 0.4217\n",
            "Epoch [4/5], Loss: 0.3962\n",
            "Epoch [5/5], Loss: 0.3783\n",
            "    Fold 2 Validation Accuracy: 86.61%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.6405\n",
            "Epoch [2/5], Loss: 0.4655\n",
            "Epoch [3/5], Loss: 0.4213\n",
            "Epoch [4/5], Loss: 0.3950\n",
            "Epoch [5/5], Loss: 0.3799\n",
            "    Fold 3 Validation Accuracy: 86.97%\n",
            "  Params: {'lr': 0.0005, 'hidden_dim': 128, 'dropout_prob': 0.3}, Avg Validation Accuracy across 3 folds: 86.94%\n",
            "\n",
            "Testing Combination 6/8: {'lr': 0.0005, 'hidden_dim': 128, 'dropout_prob': 0.5}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.7154\n",
            "Epoch [2/5], Loss: 0.5147\n",
            "Epoch [3/5], Loss: 0.4700\n",
            "Epoch [4/5], Loss: 0.4472\n",
            "Epoch [5/5], Loss: 0.4295\n",
            "    Fold 1 Validation Accuracy: 86.50%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.7088\n",
            "Epoch [2/5], Loss: 0.5143\n",
            "Epoch [3/5], Loss: 0.4705\n",
            "Epoch [4/5], Loss: 0.4455\n",
            "Epoch [5/5], Loss: 0.4284\n",
            "    Fold 2 Validation Accuracy: 86.18%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.6957\n",
            "Epoch [2/5], Loss: 0.5034\n",
            "Epoch [3/5], Loss: 0.4612\n",
            "Epoch [4/5], Loss: 0.4374\n",
            "Epoch [5/5], Loss: 0.4214\n",
            "    Fold 3 Validation Accuracy: 86.04%\n",
            "  Params: {'lr': 0.0005, 'hidden_dim': 128, 'dropout_prob': 0.5}, Avg Validation Accuracy across 3 folds: 86.24%\n",
            "\n",
            "Testing Combination 7/8: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.5946\n",
            "Epoch [2/5], Loss: 0.4392\n",
            "Epoch [3/5], Loss: 0.3980\n",
            "Epoch [4/5], Loss: 0.3718\n",
            "Epoch [5/5], Loss: 0.3528\n",
            "    Fold 1 Validation Accuracy: 87.58%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.5953\n",
            "Epoch [2/5], Loss: 0.4436\n",
            "Epoch [3/5], Loss: 0.4024\n",
            "Epoch [4/5], Loss: 0.3786\n",
            "Epoch [5/5], Loss: 0.3619\n",
            "    Fold 2 Validation Accuracy: 87.64%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.5872\n",
            "Epoch [2/5], Loss: 0.4366\n",
            "Epoch [3/5], Loss: 0.3959\n",
            "Epoch [4/5], Loss: 0.3699\n",
            "Epoch [5/5], Loss: 0.3540\n",
            "    Fold 3 Validation Accuracy: 86.98%\n",
            "  Params: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}, Avg Validation Accuracy across 3 folds: 87.40%\n",
            "  >> New best parameters found!\n",
            "\n",
            "Testing Combination 8/8: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.5}\n",
            "  Fold 1/3...\n",
            "Epoch [1/5], Loss: 0.6392\n",
            "Epoch [2/5], Loss: 0.4754\n",
            "Epoch [3/5], Loss: 0.4371\n",
            "Epoch [4/5], Loss: 0.4077\n",
            "Epoch [5/5], Loss: 0.3900\n",
            "    Fold 1 Validation Accuracy: 86.86%\n",
            "  Fold 2/3...\n",
            "Epoch [1/5], Loss: 0.6414\n",
            "Epoch [2/5], Loss: 0.4764\n",
            "Epoch [3/5], Loss: 0.4394\n",
            "Epoch [4/5], Loss: 0.4111\n",
            "Epoch [5/5], Loss: 0.3933\n",
            "    Fold 2 Validation Accuracy: 86.81%\n",
            "  Fold 3/3...\n",
            "Epoch [1/5], Loss: 0.6378\n",
            "Epoch [2/5], Loss: 0.4714\n",
            "Epoch [3/5], Loss: 0.4316\n",
            "Epoch [4/5], Loss: 0.4095\n",
            "Epoch [5/5], Loss: 0.3905\n",
            "    Fold 3 Validation Accuracy: 86.69%\n",
            "  Params: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.5}, Avg Validation Accuracy across 3 folds: 86.78%\n",
            "\n",
            "Cross-validation finished.\n",
            "Best parameters found: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}\n",
            "Best average validation accuracy: 87.40%\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "# 5. k-Fold Cross-Validation (5 points)       #\n",
        "#    + Hyperparameter Tuning & Results (5 pts) #\n",
        "###############################################\n",
        "print(\"\\n5. Implementing k-Fold Cross-Validation and Hyperparameter Tuning...\")\n",
        "\n",
        "# --- Define cross_validate function (replaces original skeleton) ---\n",
        "def cross_validate(model_class, train_dataset, k=5, param_grid=None, epochs_per_fold=5, batch_size_cv=64):\n",
        "    \"\"\"\n",
        "    Performs k-fold cross-validation to find the best hyperparameters.\n",
        "    Uses original param_grid docstring format.\n",
        "    \"\"\"\n",
        "    # --- Docstring from original template ---\n",
        "    # param_grid is a dict of hyperparameters, e.g.:\n",
        "    # {\n",
        "    #   'lr': [1e-2, 1e-3, 1e-4],\n",
        "    #   'hidden_dim': [64, 128, 256],\n",
        "    #   'dropout_prob': [0.25, 0.5]\n",
        "    # }\n",
        "    # ---\n",
        "\n",
        "    if not isinstance(param_grid, dict):\n",
        "        raise ValueError(\"param_grid must be a dictionary\")\n",
        "\n",
        "    best_params = None\n",
        "    best_accuracy = 0.0 # Changed name slightly from template\n",
        "\n",
        "    # TODO: (Original comments kept for reference, logic implemented below)\n",
        "    # (1) Shuffle and split 'train_dataset' into k folds\n",
        "    # (2) For each param combination:\n",
        "    #     For each fold:\n",
        "    #         Train on k-1 folds, validate on the remaining fold\n",
        "    #         Accumulate validation accuracy\n",
        "    #     Average the accuracy across folds\n",
        "    #     Track the best param combination\n",
        "\n",
        "    # --- Implementation Start ---\n",
        "    param_combinations = list(product(*param_grid.values()))\n",
        "    param_names = list(param_grid.keys())\n",
        "\n",
        "    print(f\"\\nStarting {k}-Fold Cross-Validation with {len(param_combinations)} parameter combinations.\")\n",
        "    print(f\"Training for {epochs_per_fold} epochs per fold.\")\n",
        "\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    for i, values in enumerate(param_combinations):\n",
        "        current_params = dict(zip(param_names, values))\n",
        "        print(f\"\\nTesting Combination {i+1}/{len(param_combinations)}: {current_params}\")\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "            print(f\"  Fold {fold+1}/{k}...\")\n",
        "            train_subset = Subset(train_dataset, train_idx)\n",
        "            val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "            # Use fold-specific loaders\n",
        "            train_loader_fold = DataLoader(train_subset, batch_size=batch_size_cv, shuffle=True)\n",
        "            val_loader_fold = DataLoader(val_subset, batch_size=batch_size_cv, shuffle=False)\n",
        "\n",
        "            # Instantiate model, filtering params\n",
        "            model_init_params = {\n",
        "                'hidden_dim': current_params.get('hidden_dim', 128),\n",
        "                'dropout_prob': current_params.get('dropout_prob', 0.5),\n",
        "                'input_dim': 784, 'num_classes': 10 # Fixed for FashionMNIST\n",
        "            }\n",
        "            model_fold = model_class(**model_init_params)\n",
        "\n",
        "            if 'lr' not in current_params:\n",
        "                 raise KeyError(\"'lr' must be in param_grid.\")\n",
        "            optimizer_fold = optim.Adam(model_fold.parameters(), lr=current_params['lr']) # Use Adam for CV\n",
        "            criterion_fold = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Train model (will print loss each epoch as per modified train_model)\n",
        "            train_model(model_fold, train_loader_fold, criterion_fold, optimizer_fold, epochs=epochs_per_fold)\n",
        "\n",
        "            # Validate\n",
        "            val_acc = evaluate_model(model_fold, val_loader_fold)\n",
        "            fold_accuracies.append(val_acc)\n",
        "            print(f\"    Fold {fold+1} Validation Accuracy: {val_acc:.2f}%\") # Indented print\n",
        "\n",
        "        # Average accuracy for this param set\n",
        "        avg_acc = np.mean(fold_accuracies)\n",
        "        print(f\"  Params: {current_params}, Avg Validation Accuracy across {k} folds: {avg_acc:.2f}%\") # Indented print\n",
        "\n",
        "        # Track best params\n",
        "        if avg_acc > best_accuracy:\n",
        "            best_accuracy = avg_acc\n",
        "            best_params = current_params\n",
        "            print(f\"  >> New best parameters found!\") # Indented print\n",
        "\n",
        "    print(f\"\\nCross-validation finished.\")\n",
        "    if best_params:\n",
        "        print(f\"Best parameters found: {best_params}\")\n",
        "        print(f\"Best average validation accuracy: {best_accuracy:.2f}%\")\n",
        "    else:\n",
        "        print(\"No best parameters found.\")\n",
        "    # --- Implementation End ---\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "# --- Define Hyperparameter Grid and Run CV ---\n",
        "param_grid_to_search = {\n",
        "    'lr': [1e-3, 5e-4],\n",
        "    'hidden_dim': [128, 256],\n",
        "    'dropout_prob': [0.3, 0.5]\n",
        "}\n",
        "num_folds = 3\n",
        "epochs_cv = 5\n",
        "\n",
        "# Call cross_validate (replaces commented-out line from template)\n",
        "best_params, best_val_acc = cross_validate(\n",
        "    SimpleFCNetwork,\n",
        "    train_dataset,\n",
        "    k=num_folds,\n",
        "    param_grid=param_grid_to_search,\n",
        "    epochs_per_fold=epochs_cv,\n",
        "    batch_size_cv=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implement k-Fold Cross-Validation (Implementation)\n",
        "\n",
        "* **Status:** Completed.  \n",
        "* **Details:** The skeleton `cross_validate` function in the template was replaced with a full implementation. It uses `sklearn.model_selection.KFold`, iterates through parameter combinations defined in a `param_grid`, trains/validates on the appropriate data subsets for each fold, averages validation accuracies, and identifies the best parameter set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6. Training Final Model with Best Parameters Found...\n",
            "Training final model on full training data for 15 epochs with params: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}\n",
            "Epoch [1/15], Loss: 0.5486\n",
            "Epoch [2/15], Loss: 0.4133\n",
            "Epoch [3/15], Loss: 0.3783\n",
            "Epoch [4/15], Loss: 0.3557\n",
            "Epoch [5/15], Loss: 0.3373\n",
            "Epoch [6/15], Loss: 0.3254\n",
            "Epoch [7/15], Loss: 0.3130\n",
            "Epoch [8/15], Loss: 0.3047\n",
            "Epoch [9/15], Loss: 0.2945\n",
            "Epoch [10/15], Loss: 0.2876\n",
            "Epoch [11/15], Loss: 0.2809\n",
            "Epoch [12/15], Loss: 0.2723\n",
            "Epoch [13/15], Loss: 0.2663\n",
            "Epoch [14/15], Loss: 0.2591\n",
            "Epoch [15/15], Loss: 0.2559\n",
            "\n",
            "7. Evaluating Final Model on Test Set...\n",
            "\n",
            "==================================================\n",
            "                Final Results Summary\n",
            "==================================================\n",
            "Hyperparameters selected via 3-Fold CV:\n",
            "  Best Parameters: {'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}\n",
            "  Best Avg. Validation Acc (CV): 87.40%\n",
            "--------------------------------------------------\n",
            "Final Model Performance:\n",
            "  Test Accuracy: 88.58%\n",
            "==================================================\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "# --- Train Final Model with Best Parameters ---\n",
        "print(\"\\n6. Training Final Model with Best Parameters Found...\") # New section header\n",
        "\n",
        "if best_params:\n",
        "    # Instantiate final model (replaces commented-out line from template)\n",
        "    final_model_init_params = {\n",
        "        'hidden_dim': best_params.get('hidden_dim', 128),\n",
        "        'dropout_prob': best_params.get('dropout_prob', 0.5),\n",
        "        'input_dim': 784, 'num_classes': 10\n",
        "    }\n",
        "    final_model = SimpleFCNetwork(**final_model_init_params)\n",
        "\n",
        "    # Create optimizer and criterion for final model\n",
        "    final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
        "    final_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train on entire train_dataset (replaces commented-out line from template)\n",
        "    final_epochs = 15 # Use more epochs for final training\n",
        "    print(f\"Training final model on full training data for {final_epochs} epochs with params: {best_params}\")\n",
        "    # Need a loader for the full training set again if original `train_loader` was modified (it wasn't here)\n",
        "    full_train_loader_final = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
        "    train_model(final_model, full_train_loader_final, final_criterion, final_optimizer, epochs=final_epochs)\n",
        "\n",
        "    # Evaluate final model (replaces commented-out line from template)\n",
        "    print(\"\\n7. Evaluating Final Model on Test Set...\") # New section header\n",
        "    test_acc = evaluate_model(final_model, test_loader)\n",
        "\n",
        "    # Print final results (replaces commented-out line from template)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"                Final Results Summary\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Hyperparameters selected via {num_folds}-Fold CV:\")\n",
        "    print(f\"  Best Parameters: {best_params}\")\n",
        "    print(f\"  Best Avg. Validation Acc (CV): {best_val_acc:.2f}%\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"Final Model Performance:\")\n",
        "    print(f\"  Test Accuracy: {test_acc:.2f}%\") # Original template format: print(\"Final Test Accuracy =\", test_acc)\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"Cross-validation did not yield best parameters. Skipping final training.\")\n",
        "\n",
        "print(\"\\nScript finished.\") # New final print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning and Final Results\n",
        "\n",
        "* **Objective:** Use 3-fold cross-validation to find the best combination of learning rate, hidden dimension size, and dropout probability from the specified grid, then report the final test accuracy using these parameters.\n",
        "\n",
        "* **Hyperparameter Grid Searched:**\n",
        "    * `lr`: [0.001, 0.0005]  \n",
        "    * `hidden_dim`: [128, 256]  \n",
        "    * `dropout_prob`: [0.3, 0.5]\n",
        "\n",
        "* **Cross-Validation Results:**\n",
        "    * **Best Parameters Found:** `{'lr': 0.0005, 'hidden_dim': 256, 'dropout_prob': 0.3}`\n",
        "    * **Best Average Validation Accuracy (during CV):** `87.40%`\n",
        "\n",
        "* **Final Model Training and Evaluation:**\n",
        "    * A final model was trained on the *entire* training dataset using the best parameters (`lr=0.0005`, `hidden_dim=256`, `dropout_prob=0.3`) for 15 epochs.\n",
        "    * **Final Test Accuracy:** `88.58%`\n",
        "\n",
        "* **Analysis:** Cross-validation successfully identified optimal hyperparameters from the grid. The best model used a larger hidden layer (256), a moderate dropout (0.3), and a slightly lower learning rate (0.0005). The final test accuracy (`88.58%`) achieved after training with these parameters improved upon the initial baseline runs and the average validation accuracy, confirming the benefit of the tuning process. The selected dropout rate (0.3) proved more effective on average than the higher 0.5 rate during cross-validation.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
