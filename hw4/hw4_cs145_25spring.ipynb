{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvsK8ij3UP_"
      },
      "source": [
        "# CS145 Introduction to Data Mining - Assignment 4  \n",
        "**Deadline: 11:59PM, May 14, 2025**\n",
        "\n",
        "## Instructions\n",
        "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
        "\n",
        "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
        "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
        "\n",
        "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
        "\n",
        "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
        "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
        "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
        "\n",
        "### Submission Requirements\n",
        "\n",
        "* Submit your solutions through GradeScope in BruinLearn.\n",
        "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t \\le 24)e^{-(\\ln(2)/12)t}$.\n",
        "\n",
        "### Collaboration and Integrity\n",
        "\n",
        "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
        "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
        "\n",
        "---\n",
        "\n",
        "## Outline\n",
        "\n",
        "- **Part 1: Write-up**\n",
        "\n",
        "  1. [EM on GMM (Proof Question)](#writeup-q1)\n",
        "\n",
        "  2. [Set Data (By-Hand Question)](#writeup-q2)\n",
        "\n",
        "  3. [CNN Convolution (By-Hand Calculation)](#writeup-q3)\n",
        "\n",
        "  4. [PrefixSpan Question](#writeup-q4)\n",
        "\n",
        "  5. [Sequence Alignment Question](#writeup-q5)\n",
        "\n",
        "- **Part 2: Coding**\n",
        "\n",
        "  6. [Gaussian Mixture Model on Real Data](#coding-q4)\n",
        "\n",
        "  7. [Implementing the Apriori Algorithm](#coding-q5)\n",
        "\n",
        "  8. [Implementing a Convolutional Neural Network (CNN)](#coding-q6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzDWSrWH3UQD"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Part 1: Write-up\n",
        "\n",
        "<a name=\"writeup-q1\"></a>\n",
        "## 1) EM Derivations for a Gaussian Mixture Model (20 points)\n",
        "\n",
        "Consider a Gaussian Mixture Model (GMM) with $K$ components, where each component $k$ has parameters $\\pi_k, \\mu_k, \\Sigma_k$. Let the **posterior probability** (responsibility) be:\n",
        "$$\n",
        "\\gamma_{nk} = p(z_n = k \\mid x_n)\n",
        "= \\frac{\\pi_k \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)}\n",
        "     {\\sum_{k'=1}^{K} \\pi_{k'} \\mathcal{N}(x_n \\mid \\mu_{k'}, \\Sigma_{k'})},\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\mathcal{N}(x \\mid \\mu, \\Sigma)\n",
        "= (2\\pi)^{-d/2}\n",
        "  |\\Sigma|^{-1/2}\n",
        "  \\exp\\left\\{-\\frac{1}{2}(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)\\right\\}.\n",
        "$$\n",
        "\n",
        "**Question**: Prove the following maximum likelihood estimates (MLE) are obtained for a GMM with soft assignments:\n",
        "$$\n",
        "\\pi_k = \\frac{\\sum_n \\gamma_{nk}}{\\sum_{k}\\sum_n \\gamma_{nk}},\n",
        "\\quad\n",
        "\\mu_k = \\frac{\\sum_n \\gamma_{nk} x_n}{\\sum_n \\gamma_{nk}},\n",
        "\\quad\n",
        "\\Sigma_k = \\frac{\\sum_n \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^\\top}{\\sum_n \\gamma_{nk}}.\n",
        "$$\n",
        "\n",
        "In your **proof**, please:\n",
        "\n",
        "1. Start from the **complete-data log-likelihood** expression (including the assignments $z_n$). (6 points)\n",
        "2. Show that maximizing w.r.t. each $\\pi_k, \\mu_k, \\Sigma_k$ yields the above formulas when $\\gamma_{nk} = p(z_n = k \\mid x_n)$. (10 points)\n",
        "3. Include **sufficient intermediate steps** in your derivation (e.g., partial derivatives, normalizing constraints). (4 points)\n",
        "\n",
        "**Hint**: You may use standard results for maximizing Gaussian likelihoods, but do show how the soft assignments $\\gamma_{nk}$ appear in place of the usual indicator variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PtG5tio3UQD"
      },
      "source": [
        "**[TODO: Write your responses here. ]**\n",
        "\n",
        "**[We have walked through this question during the discussion session. Refer to the discussion recording if you need to revisit it. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jgva5933UQE"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a name=\"writeup-q2\"></a>\n",
        "## 2) Set Data (By-Hand Question) (20 points)\n",
        "\n",
        "We have the following **5 transactions**:\n",
        "\n",
        "| TID | Items                   |\n",
        "|----:|:------------------------|\n",
        "| 10  | Beer, Nuts, Diaper     |\n",
        "| 20  | Beer, Coffee, Diaper   |\n",
        "| 30  | Beer, Diaper, Eggs     |\n",
        "| 40  | Nuts, Eggs, Milk       |\n",
        "| 50  | Nuts, Coffee, Diaper, Eggs, Milk |\n",
        "\n",
        "Assume $\\mathrm{minsup} = 50\\%$ (i.e., itemsets must appear in at least 50% of transactions).\n",
        "\n",
        "1. **Frequent 1-Itemsets** (5 points):  \n",
        "   - Count each individual item's frequency (absolute and relative). Which items are **frequent**?\n",
        "\n",
        "2. **Candidate 2-Itemsets** (7 points):  \n",
        "   - Generate all 2-itemset candidates and prune those not meeting $\\mathrm{minsup}$. Show your manual support counting.\n",
        "\n",
        "3. **3-Itemsets** (5 points):  \n",
        "   - For completeness, if any 3-itemset can be frequent under $\\mathrm{minsup} = 50\\%$, list them.\n",
        "\n",
        "4. **Brief Commentary** (3 points):  \n",
        "   - How many database scans did you perform by hand?  \n",
        "   - Could you see any **shortcuts** (like the Apriori property) that saved you from enumerating everything?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDrtR5LH3UQE"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpnPsO_3UQE"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a name=\"writeup-q3\"></a>\n",
        "## 3) CNN Convolution (By-Hand Calculation) (20 points)\n",
        "\n",
        "Consider a **single-channel** (grayscale) 5 $\\times$ 5 input image $I$ and a **single** 3 $\\times$ 3 filter $F$. Let the (row, column)-indexed pixels in $I$ be:\n",
        "\n",
        "$$\n",
        "I = \\begin{bmatrix}\n",
        "1 & 2 & 3 & 2 & 1 \\\\\n",
        "2 & 3 & 4 & 3 & 2 \\\\\n",
        "3 & 4 & 5 & 4 & 3 \\\\\n",
        "2 & 3 & 4 & 3 & 2 \\\\\n",
        "1 & 2 & 3 & 2 & 1\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "F = \\begin{bmatrix}\n",
        "1 & 0 & -1 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "-1 & 0 & 1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "We will perform a **valid convolution** (no padding), with **stride = 1**.\n",
        "\n",
        "**Task**:  \n",
        "1. Write the formula for the convolution output $O(r,c)$ for a 2D input and kernel (5 points).  \n",
        "2. Calculate **one** output cell in detail, e.g. $O(1,1)$ (using 1-based indexing for convenience). Show all multiplications and summations (7 points).  \n",
        "3. Provide the final 3 $\\times$ 3 output (8 points).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iATa-4Zq3UQE"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<a name=\"writeup-q4\"></a>\n",
        "## 4) PrefixSpan Question (20 points)\n",
        "\n",
        "**Given** the following small sequence database (with `SID` as sequence IDs), and a minimum support threshold of 2 (i.e., a subsequence must appear in at least 2 sequences to be considered frequent):\n",
        "\n",
        "| SID | Sequence                      |\n",
        "|----:|:------------------------------|\n",
        "|  1  | `<(ab) c (ac) b>`            |\n",
        "|  2  | `<(a) (bc) (ab)>`            |\n",
        "|  3  | `<(ab) a (bc) (ac)>`         |\n",
        "|  4  | `< b (ac) (ab) c>`           |\n",
        "\n",
        "Each element is shown in parentheses (e.g., `(ab)`), and within an element, items are unordered. For instance, `(ab)` is the same as `(ba)`.\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "1. **Frequent Single-Item Sequences** (5 points)\n",
        "   - Identify all length-1 (single-item) subsequences that meet the minimum support of 2.  \n",
        "   - List their support counts.\n",
        "\n",
        "2. **Prefix Projection** (7 points)\n",
        "   - Pick **one** frequent single-item prefix (e.g., `<a>` or `<b>` — whichever is frequent) and construct its **projected database**. Show **how** you derive these projected sequences (i.e., how you remove the prefix and keep the remainder as the “suffix”).\n",
        "\n",
        "3. **Frequent 2-Item Sequences** (8 points)\n",
        "   - Using the projected database from step (2), find **all** possible 2-item extensions of that prefix that are still frequent.  \n",
        "   - You do *not* need to enumerate every possible prefix in the entire database. Focus on demonstrating the prefix-projection mechanism clearly for **one** prefix.\n"
      ],
      "metadata": {
        "id": "t3vRZx0T3-Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[TODO: Write your responses here. ]**"
      ],
      "metadata": {
        "id": "t4uXQXOF4ztB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<a name=\"writeup-q5\"></a>\n",
        "## 5) Sequence Alignment Question (20 points)\n",
        "\n",
        "**Given** two DNA sequences:\n",
        "\n",
        "$$\n",
        "X = \\text{`GCATGCG`}\n",
        "$$\n",
        "$$\n",
        "Y = \\text{`CATTAGA`}\n",
        "$$\n",
        "\n",
        "Use the *Needleman-Wunsch* algorithm (global sequence alignment via dynamic programming) with the following scoring scheme:\n",
        "\n",
        "- **Match**: +1  \n",
        "- **Mismatch**: -1  \n",
        "- **Gap**: -1  \n",
        "\n",
        "(You may use any table size or approach from the lecture notes.)\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "1. **Fill Out the DP Table** (8 points)\n",
        "   - Construct an $(m+1) \\times (n+1)$ matrix (where $m$ and $n$ are the lengths of $X$ and $Y$, respectively).  \n",
        "   - Show how you compute each cell $F(i,j)$ by taking the maximum of:\n",
        "     1. $F(i-1, j) + (\\text{gap})$,  \n",
        "     2. $F(i, j-1) + (\\text{gap})$,  \n",
        "     3. $F(i-1, j-1) + s(x_i, y_j)$,  \n",
        "     where $s(x_i, y_j)$ is +1 if $x_i$ and $y_j$ match, and -1 otherwise.\n",
        "\n",
        "2. **Backtracking** (7 points)\n",
        "   - Once the table is completed, trace **back** from the bottom-right corner to retrieve **one optimal alignment** of $X$ and $Y$. Show your resulting alignment in a readable form (e.g., with dashes for gaps).\n",
        "\n",
        "3. **Final Alignment & Score** (5 points)\n",
        "   - Report the final alignment and the **optimal alignment score** $F(m,n)$.\n"
      ],
      "metadata": {
        "id": "dSpEdGZ241r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[TODO: Write your responses here. ]**"
      ],
      "metadata": {
        "id": "DfV3CzFZ74dY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA0Ze6uZ3UQF"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Coding\n",
        "\n",
        "Below are three coding assignments. You can implement them in a single Jupyter notebook or separate ones. **Please include** any textual explanations and visualizations (e.g., plots, confusion matrices) within Markdown cells in your notebook for clarity.\n",
        "\n",
        "<a name=\"coding-q4\"></a>\n",
        "## 6) Gaussian Mixture Model on Real Data (20 points)\n",
        "\n",
        "You will implement (or use a library for) a **Gaussian Mixture Model (GMM)** on the \"make_moons\" dataset from scikit-learn.\n",
        "\n",
        "**Tasks**:\n",
        "1. **Data Loading & Preprocessing** (3 points):\n",
        "   - Generate the \"make_moons\" dataset using `sklearn.datasets.make_moons`.\n",
        "   - The dataset creates two interleaving half-moon shapes, ideal for clustering visualizations.\n",
        "   - Use parameters: `n_samples=300`, `noise=0.1`, `random_state=42`.\n",
        "   - Optionally normalize or standardize features.\n",
        "\n",
        "2. **Model Training** (8 points):\n",
        "   - Either implement GMM from scratch (using E-step & M-step) **or** use an existing library (e.g., `sklearn.mixture.GaussianMixture`).\n",
        "   - Try different numbers of components $K$ (e.g., $K=2,3,4$).\n",
        "   - Experiment with different covariance types ('full', 'tied', 'diagonal', 'spherical').\n",
        "\n",
        "3. **Analysis & Visualization** (5 points):\n",
        "   - Plot the data with cluster responsibilities or predicted labels.\n",
        "   - Visualize cluster boundaries using contour plots.\n",
        "   - Print or plot the means $\\mu_k$ and mixture weights $\\pi_k$.\n",
        "\n",
        "4. **Discussion** (4 points):\n",
        "   - How did you pick the optimal $K$?  \n",
        "   - How well does GMM handle the non-Gaussian moon-shaped clusters?\n",
        "   - Compare the performance of different covariance types.\n",
        "\n",
        "**Starter Code with TODO Blocks**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QekSBoAy3UQF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "# Function to create a mesh grid for visualizing decision boundaries\n",
        "def plot_decision_boundaries(X, model, ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    # Create a mesh grid\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # TODO: Predict labels for each point in mesh and visualize decision boundaries\n",
        "    # Hint: Use model.predict() on the mesh grid points and reshape to match xx shape\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Generate the make_moons dataset\n",
        "X, y_true = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# TODO: Preprocess the data (optional but recommended)\n",
        "# Apply StandardScaler for better GMM performance\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "# X = X_scaled  # Use scaled data\n",
        "\n",
        "# Set parameters for model comparison\n",
        "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "n_components_list = [2, 3, 4]\n",
        "\n",
        "# TODO: Set up a figure for the plots\n",
        "# plt.figure(figsize=(15, 12))\n",
        "\n",
        "# TODO: Create and train multiple GMM models with different configurations\n",
        "# Loop through covariance types and number of components\n",
        "# Keep track of the best model based on adjusted_rand_score\n",
        "# best_ari = -1\n",
        "# best_model = None\n",
        "# best_config = None\n",
        "\n",
        "# TODO: For each configuration:\n",
        "# 1. Create and train a GMM model\n",
        "# 2. Get predictions\n",
        "# 3. Calculate metrics (ARI, BIC, AIC)\n",
        "# 4. Plot results\n",
        "# 5. Update best model if needed\n",
        "\n",
        "# TODO: Print details of the best model\n",
        "\n",
        "# TODO: Create a detailed visualization of the best model\n",
        "\n",
        "# TODO: Provide analysis of the results\n",
        "# 1. Discuss which K value worked best and why\n",
        "# 2. Explain how GMM handles the non-Gaussian moon shapes\n",
        "# 3. Compare different covariance types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBoCQQRf3UQG"
      },
      "source": [
        "**[TODO: Write your responses here.]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eDhrbvj3UQG"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a name=\"coding-q5\"></a>\n",
        "## 7) Implementing the Apriori Algorithm (20 points)\n",
        "\n",
        "Implement the **Apriori algorithm** for frequent itemset mining on the \"Bakery\" dataset.\n",
        "\n",
        "**Tasks**:\n",
        "1. **Load the dataset** (3 points):\n",
        "   - The \"Bakery\" dataset contains 1,000 transactions from a bakery shop, providing a manageable size for this assignment.\n",
        "   - You can download it from this URL: https://raw.githubusercontent.com/ngjiawaie/Extended_Bakery_Dataset/master/1000i.csv\n",
        "   - Each row represents a transaction with three columns: Transaction ID, Item, and Quantity.\n",
        "\n",
        "2. **Implement Apriori** from scratch (10 points):\n",
        "   - Generate 1-itemsets and count their frequencies\n",
        "   - For each k > 1, generate candidate k-itemsets from (k-1)-itemsets\n",
        "   - Prune candidates using the Apriori property\n",
        "   - Calculate support for remaining candidates\n",
        "   - Continue until no frequent itemsets are found\n",
        "\n",
        "3. **Output** (4 points):\n",
        "   - Print the **frequent itemsets** discovered (above your chosen minimum support threshold)\n",
        "   - Generate **association rules** with their confidence/lift metrics\n",
        "\n",
        "4. **Comment** (3 points):\n",
        "   - Analyze the most interesting rules you discovered\n",
        "   - Explain how the minimum support threshold affects your results\n",
        "\n",
        "**Starter Code (Apriori)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3O2XuRT3UQG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Load the Bakery dataset\n",
        "def load_bakery_dataset(filepath):\n",
        "    \"\"\"\n",
        "    Load the Bakery dataset from a CSV file.\n",
        "    Returns a list of transactions, where each transaction is a list of items.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to load from local file first\n",
        "        df = pd.read_csv(filepath, header=None, names=['TransactionID', 'Item', 'Quantity'])\n",
        "    except FileNotFoundError:\n",
        "        # If file not found, download it\n",
        "        print(f\"File not found: {filepath}. Attempting to download...\")\n",
        "        url = \"https://raw.githubusercontent.com/ngjiawaie/Extended_Bakery_Dataset/master/1000i.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            content = StringIO(response.text)\n",
        "            df = pd.read_csv(content, header=None, names=['TransactionID', 'Item', 'Quantity'])\n",
        "            print(f\"Dataset downloaded successfully.\")\n",
        "            # Save the file locally for future use\n",
        "            with open(filepath, 'w') as f:\n",
        "                f.write(response.text)\n",
        "        else:\n",
        "            raise Exception(f\"Failed to download dataset: Status code {response.status_code}\")\n",
        "\n",
        "    # Group by TransactionID and collect items into lists\n",
        "    transactions = df.groupby('TransactionID')['Item'].apply(list).tolist()\n",
        "\n",
        "    return transactions, df\n",
        "\n",
        "def generate_candidates(frequent_itemsets_k_minus_1, k):\n",
        "    \"\"\"\n",
        "    Generate candidate k-itemsets from frequent (k-1)-itemsets\n",
        "    \"\"\"\n",
        "    candidates = set()\n",
        "\n",
        "    # TODO: Implement candidate generation\n",
        "    # For k=2, generate pairs from individual items\n",
        "    # For k>2, use the apriori principle: Two (k-1)-itemsets can be joined if they share k-2 items\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def prune_candidates(candidates, frequent_itemsets_k_minus_1, k):\n",
        "    \"\"\"\n",
        "    Prune candidate k-itemsets using the Apriori property:\n",
        "    All subsets of a frequent itemset must also be frequent\n",
        "    \"\"\"\n",
        "    pruned_candidates = set()\n",
        "\n",
        "    # TODO: Implement candidate pruning\n",
        "    # Generate all (k-1)-sized subsets of candidates\n",
        "    # Keep candidate only if all its (k-1)-subsets are frequent\n",
        "\n",
        "    return pruned_candidates\n",
        "\n",
        "def count_itemsets_support(transactions, candidates):\n",
        "    \"\"\"\n",
        "    Count the support for each candidate itemset\n",
        "    \"\"\"\n",
        "    support_count = defaultdict(int)\n",
        "\n",
        "    # TODO: Implement support counting\n",
        "    # Count occurrences of each candidate in all transactions\n",
        "\n",
        "    return support_count\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    \"\"\"\n",
        "    Implement the Apriori algorithm\n",
        "\n",
        "    Parameters:\n",
        "    - transactions: List of transactions, where each transaction is a list of items\n",
        "    - min_support: Minimum support threshold (between 0 and 1)\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary of frequent itemsets with their support values\n",
        "    \"\"\"\n",
        "    # Get unique items in the dataset\n",
        "    unique_items = set()\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            unique_items.add(item)\n",
        "\n",
        "    print(f\"Dataset has {len(transactions)} transactions with {len(unique_items)} unique items.\")\n",
        "\n",
        "    # Generate 1-itemsets\n",
        "    itemsets = {frozenset([item]): 0 for item in unique_items}\n",
        "\n",
        "    # TODO: Count support for 1-itemsets\n",
        "    # Iterate through transactions and count occurrences of each item\n",
        "\n",
        "    # Get frequent 1-itemsets\n",
        "    min_count = min_support * len(transactions)\n",
        "    frequent_itemsets = {k: v for k, v in itemsets.items() if v >= min_count}\n",
        "\n",
        "    # TODO: Implement the main Apriori loop\n",
        "    # Start with k=2 and repeat until no more frequent itemsets are found:\n",
        "    # 1. Generate k-itemset candidates\n",
        "    # 2. Prune candidates\n",
        "    # 3. Count support\n",
        "    # 4. Filter to get frequent k-itemsets\n",
        "    # 5. Update result and increment k\n",
        "\n",
        "    # Track statistics for visualization\n",
        "    result = dict(frequent_itemsets)  # Initialize with 1-itemsets\n",
        "    stats = {\n",
        "        'k': [1],\n",
        "        'candidates': [len(itemsets)],\n",
        "        'frequent': [len(frequent_itemsets)]\n",
        "    }\n",
        "\n",
        "    return result, stats\n",
        "\n",
        "def generate_association_rules(frequent_itemsets, transactions, min_confidence, min_lift=1.0):\n",
        "    \"\"\"\n",
        "    Generate association rules from frequent itemsets\n",
        "\n",
        "    Parameters:\n",
        "    - frequent_itemsets: Dictionary of frequent itemsets with their support\n",
        "    - transactions: List of transactions\n",
        "    - min_confidence: Minimum confidence threshold (between 0 and 1)\n",
        "    - min_lift: Minimum lift threshold (greater than or equal to 1.0)\n",
        "\n",
        "    Returns:\n",
        "    - List of rules as tuples (antecedent, consequent, support, confidence, lift)\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    total_transactions = len(transactions)\n",
        "\n",
        "    # TODO: Implement association rule generation\n",
        "    # 1. Consider only itemsets with at least 2 items\n",
        "    # 2. For each itemset, generate all possible non-empty proper subsets as antecedents\n",
        "    # 3. Calculate support, confidence, and lift for each rule\n",
        "    # 4. Filter rules based on min_confidence and min_lift\n",
        "    # 5. Sort rules by lift (descending)\n",
        "\n",
        "    return rules\n",
        "\n",
        "def visualize_results(stats, frequent_itemsets, rules):\n",
        "    \"\"\"\n",
        "    Visualize the results of the Apriori algorithm\n",
        "    \"\"\"\n",
        "    # Create a figure with subplots\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # TODO: Implement visualization\n",
        "    # 1. Plot statistics (candidates and frequent itemsets vs k)\n",
        "    # 2. Plot top frequent items\n",
        "    # 3. Visualize rules if any were found\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('apriori_visualization.png')\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Load the Bakery dataset\n",
        "    print(\"Loading Bakery dataset...\")\n",
        "    transactions, bakery_df = load_bakery_dataset(\"1000i.csv\")\n",
        "\n",
        "    # Set parameters\n",
        "    min_support = 0.02  # 2% minimum support\n",
        "    min_confidence = 0.5  # 50% minimum confidence\n",
        "    min_lift = 1.1  # Minimum lift threshold\n",
        "\n",
        "    # Run Apriori algorithm\n",
        "    print(f\"\\nRunning Apriori algorithm with min_support={min_support}...\")\n",
        "    frequent_itemsets, stats_df = apriori(transactions, min_support)\n",
        "\n",
        "    # Print frequent itemsets\n",
        "    print(\"\\nFrequent Itemsets:\")\n",
        "    print(f\"Found {len(frequent_itemsets)} frequent itemsets in total\")\n",
        "\n",
        "    # Generate association rules\n",
        "    print(f\"\\nGenerating association rules with min_confidence={min_confidence}, min_lift={min_lift}...\")\n",
        "    rules = generate_association_rules(frequent_itemsets, transactions, min_confidence, min_lift)\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    visualize_results(stats_df, frequent_itemsets, rules)\n",
        "\n",
        "    # Add your analysis of the results\n",
        "    print(\"\\n----- Your Analysis Goes Here -----\")\n",
        "    # TODO: Add analysis of frequent itemsets and rules\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJLzWV-3UQH"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSsr_yHm3UQH"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a name=\"coding-q6\"></a>\n",
        "## 8) Implementing a Convolutional Neural Network (CNN) (20 points)\n",
        "\n",
        "Use **PyTorch** (or another deep learning framework) to build a **simple CNN** for classification on a small image dataset (e.g., MNIST-like, CIFAR-10 subset, or any small custom dataset).\n",
        "\n",
        "**Tasks**:\n",
        "1. **Data Loading** (3 points):\n",
        "   - Download or load a small dataset of images (e.g., from `torchvision.datasets`).\n",
        "   - Split into train and test sets.\n",
        "\n",
        "2. **Model Definition** (7 points):\n",
        "   - Define a CNN with at least **one convolutional layer**, one pooling layer, and one fully connected layer at the end.\n",
        "\n",
        "3. **Training & Evaluation** (7 points):\n",
        "   - Train for a few epochs, print out training loss.\n",
        "   - Evaluate on a test set, print out accuracy.\n",
        "\n",
        "4. **Discussion** (3 points):\n",
        "   - Did your CNN overfit on a small dataset?\n",
        "   - (Optional) Experiment with more layers or data augmentation.\n",
        "\n",
        "**Starter Code**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZqXXn5E3UQH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # TODO: Define your CNN architecture\n",
        "        # 1. Add at least one convolutional layer\n",
        "        # 2. Add at least one pooling layer\n",
        "        # 3. Add at least one fully connected layer\n",
        "        # Remember to specify input/output dimensions appropriate for your dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "        # Connect the layers defined in __init__\n",
        "        pass\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"\n",
        "    Load and prepare the MNIST dataset\n",
        "    \"\"\"\n",
        "    # TODO: Define data transformations and load dataset\n",
        "    # 1. Set up appropriate transforms (ToTensor, Normalize, etc.)\n",
        "    # 2. Load training and test datasets\n",
        "    # 3. Create data loaders with appropriate batch sizes\n",
        "    # 4. Get a batch of examples for visualization\n",
        "\n",
        "    return train_loader, test_loader, example_data, example_targets\n",
        "\n",
        "def visualize_data(example_data, example_targets):\n",
        "    \"\"\"\n",
        "    Visualize sample images from the dataset\n",
        "    \"\"\"\n",
        "    # TODO: Create a plot to visualize sample images with their labels\n",
        "    pass\n",
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
        "    \"\"\"\n",
        "    Train the CNN model\n",
        "    \"\"\"\n",
        "    # TODO: Implement model training\n",
        "    # 1. Define loss function and optimizer\n",
        "    # 2. Set up training loop with epochs and batches\n",
        "    # 3. In each epoch:\n",
        "    #    - Train the model (forward, loss, backward, optimize)\n",
        "    #    - Evaluate on test set\n",
        "    #    - Track metrics\n",
        "    # 4. Return training statistics\n",
        "\n",
        "    return train_losses, test_losses, accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set\n",
        "    \"\"\"\n",
        "    # TODO: Implement model evaluation\n",
        "    # 1. Set model to evaluation mode\n",
        "    # 2. Calculate loss and accuracy on test set\n",
        "    # 3. Return metrics\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def visualize_results(train_losses, test_losses, accuracies):\n",
        "    \"\"\"\n",
        "    Visualize training results\n",
        "    \"\"\"\n",
        "    # TODO: Create plots to show:\n",
        "    # 1. Training and test loss over epochs\n",
        "    # 2. Test accuracy over epochs\n",
        "    pass\n",
        "\n",
        "def main():\n",
        "    # Load and visualize data\n",
        "    train_loader, test_loader, example_data, example_targets = load_and_prepare_data()\n",
        "    visualize_data(example_data, example_targets)\n",
        "\n",
        "    # Create the model\n",
        "    model = SimpleCNN().to(device)\n",
        "    print(model)\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, test_losses, accuracies = train_model(model, train_loader, test_loader)\n",
        "\n",
        "    # Visualize training results\n",
        "    visualize_results(train_losses, test_losses, accuracies)\n",
        "\n",
        "    # Final evaluation\n",
        "    final_loss, final_accuracy = evaluate_model(model, test_loader)\n",
        "    print(f\"\\nFinal accuracy on test set: {final_accuracy:.2f}%\")\n",
        "\n",
        "    # Discussion of results\n",
        "    print(\"\\n----- Discussion -----\")\n",
        "    # TODO: Add your discussion here addressing:\n",
        "    # 1. Model architecture choice\n",
        "    # 2. Analysis of results (accuracy, overfitting)\n",
        "    # 3. Potential improvements\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVp59L993UQH"
      },
      "source": [
        "**[TODO: Write your responses here. ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcVSbBz53UQH"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}